{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "operator = 'add'\n",
    "(input_train, input_dev, input_test, \n",
    "           target_train, target_dev, target_test) = utils.import_data(operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the training dataset takes all examples, then the dev and test datasets are the same as the training one. \n",
    "if input_dev.shape[0] == 0:\n",
    "    input_dev = input_train\n",
    "    target_dev = target_train\n",
    "    input_test = input_train\n",
    "    target_test = target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65536, 16)\n",
      "(65536, 16)\n",
      "(65536, 16)\n",
      "(65536, 9)\n",
      "(65536, 9)\n",
      "(65536, 9)\n"
     ]
    }
   ],
   "source": [
    "print(input_train.shape)\n",
    "print(input_dev.shape)\n",
    "print(input_test.shape)\n",
    "print(target_train.shape)\n",
    "print(target_dev.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Convert 1D data to 2D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should get `(65536, 2, 8)` input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65536, 2, 8, 1)\n",
      "(65536, 2, 8, 1)\n",
      "(65536, 2, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "input_train = utils.get_2d_inputs(input_train)\n",
    "input_dev = utils.get_2d_inputs(input_dev)\n",
    "input_test = utils.get_2d_inputs(input_test)\n",
    "print(input_train.shape)\n",
    "print(input_dev.shape)\n",
    "print(input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 How to break down the training set to the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Break down the training set with the defined batch size.\n",
    "2. Give away the last batch if it does not have the batch size.\n",
    "3. Start a new epoch.\n",
    "4. Shuffle the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_saved_models = 'saved_models'\n",
    "rootdir_logs = 'tf_logs'\n",
    "\n",
    "train_print_period = 100\n",
    "dev_print_period = 1000\n",
    "\n",
    "# Contants\n",
    "INPUT_DIM = input_train.shape[1] \n",
    "OUTPUT_DIM = target_train.shape[1]\n",
    "\n",
    "# Hyperparameters - training\n",
    "batch_size = 32\n",
    "n_epoch = 999999999999\n",
    "str_optimizer = 'adam'\n",
    "learning_rate = 0.001\n",
    "all_correct_stop = True\n",
    "\n",
    "# Hyperparameters - model\n",
    "nn_model_type = 'cnn' # mlp, cnn, rnn\n",
    "activation = tf.nn.sigmoid\n",
    "## [filter_height, filter_width, in_channels, out_channels]\n",
    "f1_shape = (2, 1, input_train.shape[3], 16) # >=2\n",
    "f2_shape = (1, 2, f1_shape[3], 32) # >=3\n",
    "f3_shape = (1, 2, f2_shape[3], 64) # >=5\n",
    "f4_shape = (1, 2, f3_shape[3], OUTPUT_DIM)\n",
    "\n",
    "# Variables determined by other variables\n",
    "train_size = input_train.shape[0]\n",
    "n_batch = train_size // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a computational graph for the convolutional neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model structure**\n",
    "* **Input**: `input`\n",
    "  * Input size: 2x8\n",
    "  * The first 8 inputs: the first operand of addition\n",
    "  * The last 8 inputs: the second operand of addtion\n",
    "* **First layer**: `h1`\n",
    "  * filter size = 2x1\n",
    "  * stride = 1\n",
    "  * valid convolution\n",
    "  * filters = h1_fn (2 seems to be enough.)\n",
    "  * h1 size = 1 x 8 x h1_fn\n",
    "  * `h1 = tf.sigmoid(tf.nn.conv2d(input, W1, 1, 0) + b1, name='h1')`\n",
    "* **Zero padding layer**: `h1_padded`\n",
    "  * `h1_padded = tf.pad(h1, tf.constant([[0, 0], [1, 0]]), \"CONSTANT\", constant_values=0, name='h1_padded')`\n",
    "* **Second layer**: `h2` (output layer)\n",
    "  * filter size = 1x1x1\n",
    "  * stride = 1\n",
    "  * valid convolution\n",
    "  * filters = 1\n",
    "  * h2 size = 2 x 2 x h2_fn\n",
    "  * `h2 = tf.sigmoid(tf.nn.conv2d(h1_padded, W2, 1, 0) + b2, name='h2')`\n",
    "* Output size: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accessiblility to the variables**\n",
    "* I should know the weights, bias, and activations of the trained model to analyze. \n",
    "  * The weight and bias can be accessed by `tf.get_variable` with name scope.\n",
    "  * Activations are easily accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "## https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer\n",
    "if activation == tf.nn.relu:\n",
    "    init_factor = 2.0\n",
    "if activation == tf.nn.sigmoid:\n",
    "    init_factor = 1.0\n",
    "if activation == tf.nn.tanh:\n",
    "    init_factor = 1.0\n",
    "    \n",
    "fan_in_1 = f1_shape[0] * f1_shape[1] * f1_shape[2]\n",
    "fan_in_2 = f2_shape[0] * f2_shape[1] * f2_shape[2]\n",
    "fan_in_3 = f3_shape[0] * f3_shape[1] * f3_shape[2]\n",
    "fan_in_4 = f4_shape[0] * f4_shape[1] * f4_shape[2]\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal(f1_shape, stddev=np.sqrt(init_factor / fan_in_1)), name=\"W1\")\n",
    "b1 = tf.zeros((f1_shape[3]), name=\"b1\")\n",
    "W2 = tf.Variable(tf.truncated_normal(f2_shape, stddev=np.sqrt(init_factor / fan_in_2)), name=\"W2\")                 \n",
    "b2 = tf.zeros((f2_shape[3]), name=\"b2\")\n",
    "W3 = tf.Variable(tf.truncated_normal(f3_shape, stddev=np.sqrt(init_factor / fan_in_3)), name=\"W3\")\n",
    "b3 = tf.zeros((f3_shape[3]), name=\"b3\")\n",
    "W4 = tf.Variable(tf.truncated_normal(f4_shape, stddev=np.sqrt(init_factor / fan_in_4)), name=\"W4\")\n",
    "b4 = tf.zeros((f4_shape[3]), name=\"b4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NN structure\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, input_train.shape[1], input_train.shape[2], input_train.shape[3]), name='inputs') # None for mini-batch size\n",
    "targets = tf.placeholder(tf.float32, shape=(None, target_train.shape[1]), name='targets')\n",
    "            \n",
    "h1 = activation(tf.nn.conv2d(inputs, W1, strides=[1, 1, 1, 1], padding=\"VALID\") + b1, name='h1')\n",
    "h2 = activation(tf.nn.conv2d(h1, W2, strides=[1, 1, 2, 1], padding=\"VALID\") + b2, name='h2')\n",
    "h3 = activation(tf.nn.conv2d(h2, W3, strides=[1, 1, 2, 1], padding=\"VALID\") + b3, name='h3')\n",
    "\n",
    "last_logits = tf.squeeze(tf.nn.conv2d(h3, W4, strides=[1, 1, 2, 1], padding=\"VALID\") + b4, name='last_logits')\n",
    "outputs = tf.sigmoid(last_logits, name='outputs')\n",
    "\n",
    "predictions = utils.tf_tlu(outputs, name='predictions')\n",
    "\n",
    "# training epoch\n",
    "training_epoch = tf.placeholder(tf.float32, shape=None, name='training_epoch') \n",
    "\n",
    "# Loss: objective function\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=last_logits) # https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Accuracy\n",
    "(accuracy, n_wrong, n_correct) = utils.get_measures(targets, predictions)\n",
    "\n",
    "# Training, optimization\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run a session for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging to observe loss and accuracy for train, dev, and test sets\n",
    "run_id = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "logdir = '{}/{}/{}/run-{}/'.format(rootdir_logs, operator, nn_model_type, run_id)\n",
    "\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "acc_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "n_wrong_summary = tf.summary.scalar('n_wrong', n_wrong)\n",
    "\n",
    "epoch_summary = tf.summary.scalar('epoch', training_epoch)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "train_summary_writer = tf.summary.FileWriter(logdir + '/train', graph=tf.get_default_graph())\n",
    "dev_summary_writer = tf.summary.FileWriter(logdir + '/dev')\n",
    "test_summary_writer = tf.summary.FileWriter(logdir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run_id: 20180929173017\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2c2a2d29fc76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy],\n\u001b[0;32m---> 22\u001b[0;31m                                        feed_dict={inputs:batch_input, targets:batch_output, training_epoch:float_epoch})\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_print_period\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Run_id: %s\" % run_id)\n",
    "is_all_correct = False\n",
    "\n",
    "model_saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        input_train, target_train = utils.shuffle_np_arrays(input_train, target_train)\n",
    "        \n",
    "        for i_batch in range(n_batch):\n",
    "            step = n_batch * epoch + i_batch\n",
    "            float_epoch = epoch + float(i_batch) / n_batch \n",
    "            \n",
    "            \n",
    "            batch_input, batch_output = utils.get_batch(i_batch, batch_size, input_train, target_train)\n",
    "            _, train_loss, train_accuracy = sess.run([train_op, loss, accuracy],\n",
    "                                       feed_dict={inputs:batch_input, targets:batch_output, training_epoch:float_epoch})\n",
    "            \n",
    "            if step % train_print_period == 0:\n",
    "                # Run computing train loss, accuracy\n",
    "                train_loss, train_accuracy, merged_summary_op_val = sess.run(\n",
    "                    [loss, accuracy, merged_summary_op],\n",
    "                    feed_dict={inputs:batch_input, targets:batch_output, training_epoch:float_epoch})\n",
    "                #print(\"epoch: {}, step: {}, train_loss: {}, train_accuracy: {}\".format(epoch, step, train_loss, train_accuracy))\n",
    "                train_summary_writer.add_summary(merged_summary_op_val, step)\n",
    "                \n",
    "                    \n",
    "            if step % dev_print_period == 0 or i_batch == n_batch - 1:\n",
    "                # i_batch == n_batch - 1: The last batch = The end of an epoch \n",
    "                # Run computing dev loss, accuracy\n",
    "                dev_loss, dev_accuracy, merged_summary_op_val, n_wrong_val = sess.run(\n",
    "                    [loss, accuracy, merged_summary_op, n_wrong],\n",
    "                    feed_dict={inputs:input_dev, targets:target_dev, training_epoch:float_epoch})\n",
    "                \n",
    "                #print(\"└ epoch: {}, step: {}, dev_loss: {}, dev_accuracy: {}, n_wrong: {}\".format(epoch, step, dev_loss, dev_accuracy, n_wrong_val))\n",
    "                dev_summary_writer.add_summary(merged_summary_op_val, step)\n",
    "        \n",
    "                # Save the trained model\n",
    "                # End of one epoch OR trained with 100% accuracy\n",
    "                if i_batch == n_batch - 1:\n",
    "                    model_name = 'epoch{}-batch{}'.format(epoch, i_batch)\n",
    "                    model_saver.save(sess, '{}/{}/{}/{}/{}.ckpt'.format(\n",
    "                        dir_saved_models, operator, nn_model_type, run_id, model_name))\n",
    "                    #print(\"Model saved.\")\n",
    "                    \n",
    "                if 50 <= n_wrong_val and n_wrong_val < 100:\n",
    "                    dev_print_period = 100\n",
    "                if 10 <= n_wrong_val and n_wrong_val < 50:\n",
    "                    dev_print_period = 10\n",
    "                if n_wrong_val < 10:\n",
    "                    dev_print_period = 1\n",
    "                    \n",
    "                ##\n",
    "                # If there is no wrong operation, then ...\n",
    "                if n_wrong_val == 0 and all_correct_stop:\n",
    "                    is_all_correct = True\n",
    "                    break # Break the batch for-loop\n",
    "                    \n",
    "                 \n",
    "            \n",
    "        # End of one epoch\n",
    "        if is_all_correct and all_correct_stop:\n",
    "            break # Break the epoch for-loop\n",
    "                    \n",
    "    # End of all epochs \n",
    "    # Run computing test loss, accuracy\n",
    "    test_loss, test_accuracy, merged_summary_op_val, n_wrong_val = sess.run(\n",
    "        [loss, accuracy, merged_summary_op, n_wrong],\n",
    "        feed_dict={inputs:input_dev, targets:target_dev, training_epoch:float_epoch})\n",
    "    \n",
    "    #print(\"└ epoch: {}, step: {}, test_loss: {}, test_accuracy: {}, n_wrong: {}\".format(epoch, step, test_loss, test_accuracy, n_wrong_val))\n",
    "    test_summary_writer.add_summary(merged_summary_op_val, step)\n",
    "    \n",
    "    model_saver.save(sess, '{}/{}/{}/{}/{}.ckpt'.format(\n",
    "        dir_saved_models, operator, nn_model_type, run_id, run_id))\n",
    "    #print(\"Model saved.\")\n",
    "\n",
    "train_summary_writer.close()\n",
    "dev_summary_writer.close()    \n",
    "test_summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Restore a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model_to_import = '20180808171010-fnn-relu-256-adam-lr0.001000-bs32-testacc0.999.ckpt'\n",
    "\n",
    "saver = tf.train.import_meta_graph('{}/{}/{}.meta'.format(dir_saved_models, run_id, model_to_import))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, '{}/{}/{}'.format(dir_saved_model, run_id, model_to_import))\n",
    "    \n",
    "    # End of all epochs \n",
    "    # Run computing test loss, accuracy\n",
    "    test_loss, summary, test_accuracy = sess.run(\n",
    "        [loss, merged_summary_op, accuracy],\n",
    "        feed_dict={inputs:input_dev, targets:target_dev})\n",
    "    \n",
    "    print(\"└ test_loss: {}, test_accuracy: {}\".format(test_loss, test_accuracy))'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
